{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synth training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params & configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/btadeusz/miniconda3/envs/ddsp/lib/python3.11/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "from ddsp import DDSP, AudioDataset, CLAPLoss\n",
    "from ddsp.callbacks import BetaWarmupCallback\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import Audio\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import lightning as L\n",
    "import torch\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "torch.set_default_device('cuda')\n",
    "\n",
    "from random import randint\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Model parameters\n",
    "# model_name = 'liget'\n",
    "model_name = 'drums'\n",
    "fs = 44100\n",
    "n_signal = 1.5 * fs # 1.5 seconds\n",
    "training_path_root = '/home/btadeusz/code/ddsp_vae/training/seven-manifolds'\n",
    "models_path_root = '/home/btadeusz/code/ddsp_vae/models/seven-manifolds'\n",
    "# dataset_path = '/mnt/mariadata/datasets/seven_manifolds/liget'\n",
    "dataset_path = '/mnt/mariadata/datasets/seven_manifolds/drums'\n",
    "\n",
    "# DDSP parameters\n",
    "n_filters = 768\n",
    "n_sines = 0\n",
    "latent_size = 8\n",
    "\n",
    "# Training config\n",
    "warmup_start = 300\n",
    "warmup_end = 500\n",
    "beta = 0\n",
    "max_epochs = 500\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Training dir\n",
    "training_path = os.path.join(training_path_root, model_name)\n",
    "synth_output_path = os.path.join(models_path_root, f'{model_name}.ts')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitiate the training path\n",
    "shutil.rmtree(training_path, ignore_errors=True)\n",
    "os.makedirs(training_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = AudioDataset(dataset_path=dataset_path, n_signal=n_signal, sampling_rate=fs)\n",
    "\n",
    "train_set, val_set = random_split(dataset, [0.9, 0.1], generator=torch.Generator(device='cuda'))\n",
    "train_loader = DataLoader(train_set, batch_size=16, shuffle=True, num_workers=0, generator=torch.Generator(device='cuda'))\n",
    "val_loader = DataLoader(val_set, batch_size=16, shuffle=False, num_workers=0, generator=torch.Generator(device='cuda'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model initialisation and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "ddsp = DDSP(\n",
    "  n_filters=n_filters,\n",
    "  n_sines=n_sines,\n",
    "  fs=fs,\n",
    "  latent_size=latent_size,\n",
    "  learning_rate=learning_rate\n",
    ").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "## Callbacks\n",
    "training_callbacks = []\n",
    "\n",
    "beta_warmup = BetaWarmupCallback(\n",
    "  beta=beta,\n",
    "  start_steps=warmup_start,\n",
    "  end_steps=warmup_end\n",
    ")\n",
    "training_callbacks.append(beta_warmup)\n",
    "\n",
    "# last_checkpoint_callback = ModelCheckpoint(\n",
    "#     filename='last',\n",
    "#     save_top_k=1,  # Save only one file, the most recent one\n",
    "#     save_last=True  # Always save the model at the end of the epoch\n",
    "#   )\n",
    "# training_callbacks.append(last_checkpoint_callback)\n",
    "\n",
    "# best checkopint callback, given the validation loss\n",
    "best_checkpoint_callback = ModelCheckpoint(\n",
    "  filename='best',\n",
    "  monitor='val_loss',\n",
    "  mode='min',\n",
    "  save_top_k=1,  # Save only one file, the best one\n",
    ")\n",
    "training_callbacks.append(best_checkpoint_callback)\n",
    "\n",
    "## Trainer\n",
    "tb_logger = TensorBoardLogger(training_path_root, name=model_name)\n",
    "\n",
    "# from lightning.pytorch.profilers import PyTorchProfiler\n",
    "# profiler = PyTorchProfiler(with_stack=True)\n",
    "\n",
    "trainer = L.Trainer(\n",
    "  callbacks=training_callbacks,\n",
    "  max_epochs=max_epochs,\n",
    "  accelerator='cuda',\n",
    "  precision=16,\n",
    "  log_every_n_steps=0,\n",
    "  logger=tb_logger,\n",
    "  # profiler=profiler\n",
    ")\n",
    "\n",
    "## Start training\n",
    "trainer.fit(\n",
    "  model=ddsp,\n",
    "  train_dataloaders=train_loader,\n",
    "  val_dataloaders=val_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddsp = ddsp.cuda()\n",
    "from random import randint\n",
    "test_x = dataset[randint(0, len(dataset))].unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "  test_y = ddsp(test_x)\n",
    "\n",
    "print(\"Input\")\n",
    "display(Audio(test_x.squeeze(0).cpu().numpy(), rate=fs))\n",
    "\n",
    "print(\"Output\")\n",
    "display(Audio(test_y.squeeze().cpu().numpy(), rate=fs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning with CLAP loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Freeze parameters\n",
    "# ddsp = ddsp.cuda()\n",
    "# ddsp.train()\n",
    "# # ddsp.encoder.eval()\n",
    "# for param in ddsp.encoder.parameters():\n",
    "#   param.requires_grad = False\n",
    "\n",
    "# ddsp._recons_loss = CLAPLoss()\n",
    "\n",
    "# max_epochs = 100\n",
    "# trainer = L.Trainer(\n",
    "#   callbacks=training_callbacks,\n",
    "#   max_epochs=max_epochs,\n",
    "#   accelerator='cuda',\n",
    "#   precision=16,\n",
    "#   log_every_n_steps=0,\n",
    "#   logger=tb_logger,\n",
    "#   # profiler=profiler\n",
    "# )\n",
    "\n",
    "# trainer.fit(\n",
    "#   model=ddsp,\n",
    "#   train_dataloaders=train_loader,\n",
    "#   val_dataloaders=val_loader,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the model\n",
    "!python -m cli.export --model_directory {training_path} --output_path  {synth_output_path} --type best\n",
    "\n",
    "print(f'Model saved at {synth_output_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prior training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params & configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_batch_size = 64\n",
    "d_model = 64 # model dimensionality\n",
    "sequence_length = 128\n",
    "nhead = 2\n",
    "num_layers = 6\n",
    "prior_epochs = 100\n",
    "dataset_stride_factor = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddsp.prior import Prior, PriorDataset\n",
    "\n",
    "prior_model_name = f'{model_name}-prior'\n",
    "\n",
    "# Training dir\n",
    "prior_training_path = os.path.join(training_path_root, prior_model_name)\n",
    "\n",
    "# Reinitiate the training path\n",
    "shutil.rmtree(prior_training_path, ignore_errors=True)\n",
    "os.makedirs(prior_training_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior dataset\n",
    "The Prior dataset is the same as the audio dataset encoded by the synth encoder into the latent space and arranged into sequences, ready for the sequence prediction training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding audio dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/btadeusz/miniconda3/envs/ddsp/lib/python3.11/site-packages/torch/nn/modules/module.py:1527: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "prior_dataset = PriorDataset(\n",
    "  audio_dataset_path=dataset_path,\n",
    "  encoding_model_path=synth_output_path,\n",
    "  sequence_length=sequence_length+1,\n",
    "  sampling_rate=fs,\n",
    "  device='cuda',\n",
    "  stride_factor=0.25\n",
    ")\n",
    "\n",
    "generator = torch.Generator(device='cuda')\n",
    "prior_train_set, prior_val_set = random_split(prior_dataset, [0.8, 0.2], generator=generator)\n",
    "prior_train_loader = DataLoader(prior_train_set, batch_size=prior_batch_size, shuffle=True, generator=generator)\n",
    "prior_val_loader = DataLoader(prior_val_set, batch_size=prior_batch_size, shuffle=False, generator=generator)\n",
    "\n",
    "latent_size = prior_dataset[0].shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model initialisation and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/btadeusz/miniconda3/envs/ddsp/lib/python3.11/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "/home/btadeusz/miniconda3/envs/ddsp/lib/python3.11/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: /home/btadeusz/code/ddsp_vae/training/seven-manifolds/drums-prior/drums-prior\n",
      "/home/btadeusz/miniconda3/envs/ddsp/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:652: Checkpoint directory /home/btadeusz/code/ddsp_vae/training/seven-manifolds/drums-prior exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                 | Type                        | Params | Mode \n",
      "-----------------------------------------------------------------------------\n",
      "0 | _projection          | Sequential                  | 1.1 K  | train\n",
      "1 | _encoder             | TransformerEncoder          | 1.7 M  | train\n",
      "2 | _positional_encoding | LearnablePositionalEncoding | 8.2 K  | train\n",
      "3 | _activation          | ReLU                        | 0      | train\n",
      "4 | _out                 | Linear                      | 1.0 K  | train\n",
      "5 | _loss                | MSELoss                     | 0      | train\n",
      "-----------------------------------------------------------------------------\n",
      "1.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.7 M     Total params\n",
      "6.789     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84be6a268e2342fba3a2188a76896cab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/btadeusz/miniconda3/envs/ddsp/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=27` in the `DataLoader` to improve performance.\n",
      "/home/btadeusz/miniconda3/envs/ddsp/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=27` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "199852515c3842279fdbd0982d8f280d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80bb4cfe11af446983b412f18b710da3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "878a2dfc9a9c4a45bb6bd4ef0fdcb9c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9dd7a5d7c04474fbe816c506c3a861d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/btadeusz/miniconda3/envs/ddsp/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "prior = Prior(\n",
    "  latent_size=latent_size,\n",
    "  d_model=d_model,\n",
    "  max_len=sequence_length,\n",
    "  lr=1e-3,\n",
    "  nhead=nhead,\n",
    "  num_layers=num_layers,\n",
    ")\n",
    "\n",
    "prior_logger = TensorBoardLogger(prior_training_path, name=prior_model_name)\n",
    "\n",
    "prior_callbacks = []\n",
    "prior_checkpoint_callback = ModelCheckpoint(\n",
    "  dirpath=prior_training_path,\n",
    "  filename='best',\n",
    "  monitor='val_loss',\n",
    "  mode='min'\n",
    ")\n",
    "prior_callbacks.append(prior_checkpoint_callback)\n",
    "\n",
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=1000, stopping_threshold=4e-3)\n",
    "prior_callbacks.append(early_stopping)\n",
    "\n",
    "prior_trainer = L.Trainer(\n",
    "  callbacks=prior_callbacks,\n",
    "  accelerator='cuda',\n",
    "  log_every_n_steps=4,\n",
    "  logger=prior_logger,\n",
    "  max_epochs=prior_epochs\n",
    ")\n",
    "\n",
    "# Start training\n",
    "prior_trainer.fit(\n",
    "  model=prior,\n",
    "  train_dataloaders=prior_train_loader,\n",
    "  val_dataloaders=prior_val_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_val_loss = prior_trainer.callback_metrics['val_loss']\n",
    "print(f'Final validation loss: {prior_val_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unconditional generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import jit\n",
    "synth = jit.load(synth_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Number of steps to generate\n",
    "num_steps = 15000\n",
    "\n",
    "prior = prior.train().to('cuda')\n",
    "\n",
    "# Initialize the sequence with the given random codes\n",
    "# sequence = torch.zeros(1, sequence_length, latent_size, device='cuda')\n",
    "\n",
    "sequence = prior_dataset[15].unsqueeze(0).to('cuda')\n",
    "\n",
    "# Generate latent codes autoregressively\n",
    "for _ in range(num_steps):\n",
    "  # Predict the next latent code\n",
    "  with torch.no_grad():\n",
    "    next_code = prior(sequence[:, -sequence_length:, :])\n",
    "\n",
    "  # Append the predicted code to the sequence and shift the sequence to the right\n",
    "  sequence = torch.cat((sequence, next_code[:, -1:, :]), dim=1)\n",
    "\n",
    "mu, logvar = sequence.chunk(2, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with torch.no_grad():\n",
    "  latents, _ = synth.pretrained.encoder.reparametrize(mu, logvar)\n",
    "  audio = synth.decode(latents.permute(0, 2, 1).to('cpu'))\n",
    "\n",
    "# display latents\n",
    "plt.plot(latents.squeeze(0).cpu().numpy())\n",
    "plt.show()\n",
    "\n",
    "# display audio\n",
    "audio = audio.cpu().numpy().squeeze()\n",
    "audio = audio / audio.max()\n",
    "audio_widget = Audio(audio, rate=fs)\n",
    "display(audio_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  display(Audio(dataset[2].cpu().numpy(), rate=fs))\n",
    "  display(Audio(synth.pretrained(dataset[2].unsqueeze(0).cpu()).squeeze().numpy(), rate=fs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddsp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
